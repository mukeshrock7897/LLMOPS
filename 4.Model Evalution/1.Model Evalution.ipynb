{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ✅ 3. Model Evaluation & Alignment\n",
    "\n",
    "Evaluate LLMs on correctness, coherence, bias, and alignment with task goals — using both traditional and LLM-native techniques.\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 **3.1 Traditional Metrics**\n",
    "\n",
    "Classical NLP metrics — often **limited for open-ended LLM tasks**:\n",
    "\n",
    "| Metric          | Use Case                                     |\n",
    "| --------------- | -------------------------------------------- |\n",
    "| `Perplexity`    | Language modeling fluency                    |\n",
    "| `BLEU`, `ROUGE` | Text similarity (summarization, translation) |\n",
    "| `F1-score`      | Span-based QA, classification                |\n",
    "\n",
    "> ⚠️ Limitation: Can’t judge *reasoning*, *factuality*, or *coherence* reliably.\n",
    "\n",
    "---\n",
    "\n",
    "### 🤖 **3.2 LLM-as-a-Judge**\n",
    "\n",
    "Use **LLMs themselves** to evaluate outputs:\n",
    "\n",
    "| Tool/Method           | Purpose                                   |\n",
    "| --------------------- | ----------------------------------------- |\n",
    "| `MT-Bench` (LMSYS)    | Multi-turn QA scoring via GPT-based judge |\n",
    "| `LMSYS Chatbot Arena` | Human + LLM ranking of response quality   |\n",
    "\n",
    "Useful for **ranking** or **grading** generations on helpfulness, relevance, safety.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 **3.3 Agent Evaluation**\n",
    "\n",
    "Evaluate **multi-step reasoning, planning, or code tasks**:\n",
    "\n",
    "| Metric/Tool | Use Case                           |\n",
    "| ----------- | ---------------------------------- |\n",
    "| `HumanEval` | Code correctness for LLMs (OpenAI) |\n",
    "| `CodeEval`  | Functional correctness & style     |\n",
    "| `Reflexion` | Agent retry loops to improve eval  |\n",
    "\n",
    "Works well for **task completion** agents (code, reasoning, tools).\n",
    "\n",
    "---\n",
    "\n",
    "### 📚 **3.4 RAG Evaluation**\n",
    "\n",
    "Metrics for **Retrieval-Augmented Generation**:\n",
    "\n",
    "| Metric              | Checks for                        |\n",
    "| ------------------- | --------------------------------- |\n",
    "| `Faithfulness`      | Hallucination-free generation     |\n",
    "| `Relevance`         | Retrieved docs match the question |\n",
    "| `Context Adherence` | Uses relevant context properly    |\n",
    "\n",
    "🛠 Tools: `RAGAS`, `TruLens`, `LangChainEval`\n",
    "\n",
    "---\n",
    "\n",
    "### ⚖️ **3.5 Robustness & Fairness**\n",
    "\n",
    "Stress-test LLMs for **biases, adversarial prompts, and edge cases**:\n",
    "\n",
    "| Tool   | Purpose                               |\n",
    "| ------ | ------------------------------------- |\n",
    "| `SHAP` | Feature attribution for bias analysis |\n",
    "| `LIME` | Local explanations for classification |\n",
    "\n",
    "> Add adversarial & toxic prompt testing for production-grade models.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
