{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 2.6 Infrastructure as Code (IaC)\n",
    "\n",
    "Provision, scale, and manage LLM infrastructure **declaratively** using code ‚Äî ensuring reproducibility, scalability, and cost-efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### üß± **2.6.1 Provisioning Compute**\n",
    "\n",
    "Declarative infrastructure setup for LLM training/inference:\n",
    "\n",
    "| Tool        | Purpose                                   |\n",
    "| ----------- | ----------------------------------------- |\n",
    "| `Terraform` | Provision cloud GPUs, storage, networking |\n",
    "| `Pulumi`    | IaC with Python/TypeScript support        |\n",
    "\n",
    "Common use: Automate launch of A100/TPU nodes across AWS, GCP, Azure.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚òÅÔ∏è **2.6.2 Cloud Platforms**\n",
    "\n",
    "Pre-built MLOps services for scalable deployments:\n",
    "\n",
    "| Platform        | Capabilities                                    |\n",
    "| --------------- | ----------------------------------------------- |\n",
    "| `AWS SageMaker` | Model training, inference endpoints, monitoring |\n",
    "| `GCP Vertex AI` | Pipelines, tuning, managed notebooks            |\n",
    "| `Azure ML`      | End-to-end MLOps with security & cost tracking  |\n",
    "\n",
    "Supports fine-tuning, monitoring, and deployment out of the box.\n",
    "\n",
    "---\n",
    "\n",
    "### üìÖ **2.6.3 Autoscaling & Scheduling**\n",
    "\n",
    "Manage resource scaling for training & inference jobs:\n",
    "\n",
    "| Tool         | Use Case                                    |\n",
    "| ------------ | ------------------------------------------- |\n",
    "| `Kubernetes` | Pod autoscaling, GPU orchestration          |\n",
    "| `Ray`        | Parallel model serving, dynamic autoscaling |\n",
    "| `Slurm`      | HPC job scheduling in research clusters     |\n",
    "\n",
    "Useful for batch training jobs or scaling real-time endpoints.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ **2.6.4 Reproducible Environments**\n",
    "\n",
    "Ensure experiments run identically across environments:\n",
    "\n",
    "| Tool     | Purpose                                   |\n",
    "| -------- | ----------------------------------------- |\n",
    "| `Docker` | Containerize environments for portability |\n",
    "| `Conda`  | Manage Python & CUDA dependencies         |\n",
    "\n",
    "Use environment.yaml or Dockerfile with model versions for reproducibility.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
