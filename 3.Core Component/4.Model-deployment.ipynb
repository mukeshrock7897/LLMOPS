{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 2.4 Model Deployment\n",
    "\n",
    "Deploy optimized and scalable LLMs to serve real-time, batch, or streaming requests efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **2.4.1 Inference Optimization**\n",
    "\n",
    "Speed up inference and reduce hardware costs:\n",
    "\n",
    "| Technique       | Use Case                                       |\n",
    "| --------------- | ---------------------------------------------- |\n",
    "| `Quantization`  | Reduce model size & latency (e.g., INT8, GPTQ) |\n",
    "| `vLLM`          | Fast LLM serving with PagedAttention           |\n",
    "| `ONNX` / `GGUF` | Hardware-agnostic model formats                |\n",
    "| `Model Pruning` | Remove unneeded weights for speed              |\n",
    "| `LoRA Adapters` | Efficient inference with PEFT-loaded weights   |\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ **2.4.2 Serving Frameworks**\n",
    "\n",
    "Deploy LLMs through high-performance model servers:\n",
    "\n",
    "| Tool                      | Purpose                                              |\n",
    "| ------------------------- | ---------------------------------------------------- |\n",
    "| `Triton Inference Server` | NVIDIA-optimized model serving platform              |\n",
    "| `BentoML`                 | Package & deploy models as APIs                      |\n",
    "| `Ray Serve`               | Distributed LLM inference at scale                   |\n",
    "| `TGI` (Text Gen Infer.)   | Fast text generation server (for HuggingFace models) |\n",
    "\n",
    "Choose based on scale, format (transformers, GGUF), and infra type (GPU/CPU).\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ **2.4.3 Streaming & Batching**\n",
    "\n",
    "Handle requests efficiently for latency-sensitive LLM apps:\n",
    "\n",
    "* üßµ Real-time ‚Üí `WebSockets`, `Server-Sent Events (SSE)`\n",
    "* üì¶ Batch processing ‚Üí Group requests to reduce cost\n",
    "* ‚úÖ Combine with async frameworks: `FastAPI + asyncio`, `Ray Serve batching`\n",
    "\n",
    "---\n",
    "\n",
    "### üß© **2.4.4 Endpoint Management & Versioning**\n",
    "\n",
    "Organize production endpoints and track model changes:\n",
    "\n",
    "| Component                  | Purpose                                      |\n",
    "| -------------------------- | -------------------------------------------- |\n",
    "| `FastAPI`, `Flask`, `gRPC` | Serve LLMs via REST or RPC-based APIs        |\n",
    "| `MLflow`                   | Track and version deployed models            |\n",
    "| `KServe`                   | Model versioning + autoscaling in Kubernetes |\n",
    "\n",
    "Use headers/params for version routing: `/v1/gpt`, `/v2/gemma`\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
